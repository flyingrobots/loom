---
Status: Draft**
Author: James Ross**
Contributors: JIT Community**
Requires:
- RFC-0001 Node Identity
- RFC-0002 DAG Invariants
- RFC-0006 WAL & Replay**
Start Date: 2025-11-28**
Target Spec: JITOS v0.x
License: TBD
---

# JIT RFC-0009

## Storage Tiering & Rehydration (STR v1.0)

### Thermodynamics of a Post-File Causal Universe

---

# 1. Summary

This RFC defines the multi-tiered storage model of the JIT substrate and the deterministic rehydration semantics that allow nodes to move between:

- Hot tier (fast local SSD)
- Warm tier (compressed local storage)
- Cold tier (remote object store)

Tiering allows JIT to:

- retain infinite history
- keep the graph immutable
- avoid garbage collection
- support massive datasets and deep provenance
- operate efficiently on commodity hardware
- work equally well on laptops, servers, and clusters

At the same time, rehydration ensures:

- nodes appear “present” when needed
- deterministic reconstruction
- transparent access
- no correctness loss
- no identity mutation
- flawless causal replay

This RFC describes JIT’s storage physics.

---

## 2. Motivation

JIT is an append-only causal universe.
It grows forever.
Nodes accumulate indefinitely.

But:

- SSD is finite
- RAM is finite
- compute is bounded

Therefore, JIT must support tiered storage analogous to:

- LSM-leveling
- CPU cache hierarchies
- memory tiers
- cold/warm/hot object lifecycles
- database storage pyramids

Except:

JIT cannot compact.

JIT cannot rewrite.

JIT cannot delete.

JIT cannot mutate.

So we must move, not mutate, nodes.

---

## 3. Tier Definitions

### 3.1 Hot Tier

Location:

```bash
.gitd/nodes/hot/
```

Characteristics:

- SSD-backed
- uncompressed node files
- minimal latency
- primary working set
- required for:
- active SWS
- current HEAD
- recent history
- local operations

This is “live memory.”

---

## 3.2 Warm Tier

Location:

```bash
.gitd/nodes/warm/
```

Characteristics:

- compressed chunks (CDC or zstd)
- indexed via LMDB
- locally stored but slower than SSD
- used for:
- mid-range history
- older builds
- previous snapshots
- CI data

Intermediate entropy state.

---

### 3.3 Cold Tier

Location:

```bash
remote://<object-store>/jit/<repo>/<node-id>
```

Characteristics:

- object store (S3, GCS, Backblaze, R2, MinIO, ZFS remote)
- content-addressed
- compressed
- deduplicated
- extremely cheap

Cold tier holds:

- deep history
- long-term provenance
- giant payloads
- scientific data
- old builds
- model checkpoints

This is the cryogenic freezer of truth.

---

## 4. Tier Promotion & Eviction Rules

Nodes naturally move:

Hot → Warm → Cold

Invariant:

Tier changes MUST NOT alter NodeID or content.

All rehydration MUST reconstruct the node exactly as originally created.

### 4.1 Eviction Triggers

- storage pressure
- LRU heuristics
- snapshot age
- admin command
- background tiering job
- automatic because of DAG age

Eviction is safe because:

- nodes are immutable
- refs don’t change
- DAG structure is preserved

Only location changes.

---

## 5. Rehydration Rules

When a node is requested:

- MH
- SWS
- dag.get_node
- collapse
- diff
- projection
- sync

…it MUST be loaded from tiered storage.

Rehydration algorithm:

```rust
function load_node(id):
    if hot.contains(id): return hot.read(id)
    if warm.contains(id): decompress → promote to hot → return
    if cold.contains(id): fetch → decompress → promote to warm → return
    else: error("node not found")
```

Key points:

- cold fetch → warm
- warm decompress → hot
- hot remains until eviction

JIT MUST guarantee perfect reconstruction.

---

## 6. Indexing Requirements

The LMDB index MUST store:

{id → tier_location}
{id → warm_chunk_offsets}
{id → cold_storage_url/hash}

Indexes MUST remain:

- deterministic
- durable
- recoverable via WAL + checkpoint

---

## 7. WAL Interaction

Tier movements MUST NOT be recorded in WAL.

Why?

WAL models temporal truth, not storage logistics.

Storage is contingent.
Truth is invariant.

WAL MUST NOT care if nodes are in:

- hot
- warm
- cold

Replay MUST produce identical DAG independent of tiers.

---

## 8. Distributed Sync

Cold tier is essential for distributed sync:

- nodes pushed to remote cold storage
- references updated
- peers rehydrate on demand
- no requirement to store full history locally

This enables:

- lazy clone
- partial repos
- serverless compute
- thin agents
- low-storage CI workers

---

## 9. Edge Cases & Guarantee

### 9.1 If cold storage is unreachable

Node fetch MUST fail gracefully.
System MUST NOT corrupt DAG.

### 9.2 Partial local copies

Absolutely allowed.
System MUST fetch missing nodes deterministically.

### 9.3 Compression incompatibility

Old cold nodes MUST be treated as opaque; decompressed to canonical encoding.

### 9.4 Duplicate storage

Allowed but not required.
Local caches may remain.

---

## 10. Security

Cold tier MUST:

- verify BLAKE3 hashes
- validate canonical encoding
- enforce signature checks
- reject corrupted data

Tier transitions MUST NOT alter content.

---

## 11. Why Tiering Matters

Tiering allows:

- infinite history
- safe immutability
- distributed scalability
- scientific reproducibility
- artifact longevity
- low-cost storage
- high-performance compute

Without tiering: 
JIT becomes too expensive.

With tiering: 
*JIT becomes practically infinite.*

---

## 12. Status & Next Steps

### Next RFC:

[RFC-0010](./RFC-0010.md) — Ref Management & Branch Semantics (The Truth Pointers of the Universe)

---

# **CΩMPUTER • JITOS** 
© 2025 James Ross • [Flying • Robots](https://flyingrobots.dev)
All Rights Reserved

