JIT RFC-0011

Distributed Sync: Frontier Negotiation & Subgraph Transfer (DST v1.0)

Truth Reconciliation in a Causal Universe

Status: Draft**
Author: James Ross
Contributors: JIT Community
Requires:
	•	RFC-0001 Node Identity
	•	RFC-0002 DAG Invariants
	•	RFC-0003 SWS
	•	RFC-0005 Inversion Engine
	•	RFC-0006 WAL
	•	RFC-0010 Ref Semantics**
Start Date: 2025-11-28**
Target Spec: JITOS v0.x**
License: TBD**

⸻

1. Summary

This RFC defines the Distributed Sync Protocol used by JIT to:
	•	synchronize repositories
	•	replicate causal DAGs
	•	negotiate frontiers
	•	exchange missing nodes
	•	update refs
	•	reconcile divergent histories

Unlike Git, JIT sync is:
	•	causality-aware
	•	branch-consistent
	•	rewrite-safe
	•	deterministic
	•	immutable
	•	subgraph-based

Sync becomes a geometric reconciliation between worldlines.

⸻

2. Motivation

Git sync is built for:
	•	files
	•	snapshots
	•	patches
	•	textual diffs
	•	mutation

JIT is built for:
	•	nodes
	•	causal chains
	•	rewrites
	•	collapse events
	•	worldlines

The protocols cannot be the same.

Distributed systems demand:
	•	efficient frontier comparison
	•	safe transfer of immutable nodes
	•	conflict-safe ref updates
	•	deterministic reconstruction
	•	efficient rehydration
	•	precise provenance integrity

This RFC defines the rules.

⸻

3. Core Concepts

3.1 Frontier

A frontier is a set of nodes representing the “tips” of the local universe.

Formally:

frontier = { f | f ∈ DAG and ∄ child such that f is a parent of child }

Comparable to Git’s “heads”, but purely causal, not Git’s commit model.

⸻

3.2 Delta Set

The delta is the set of nodes one peer has that the other does not.

delta(A → B) = nodes(A) - nodes(B)

This must be computed without sending full DAGs.

⸻

3.3 Subgraph Transfer

Nodes transfer in closure form:

subgraph(root) = { all reachable ancestors of root }

Transfers MUST send:
	•	the snapshot node
	•	any rewrite nodes
	•	file-chunk nodes
	•	provenance nodes
	•	metadata

Never only partial information.

⸻

4. Sync Stages

Sync occurs in 3 stages:

⸻

4.1 Stage 1: Frontier Exchange

Peers exchange:

local_frontier = list<NodeID>
refs = list<Ref>
capabilities = list<FeatureFlags>

Comparison determines:
	•	common ancestors
	•	divergence points
	•	delta sets
	•	missing subgraphs

Frontier negotiation MUST be efficient (logarithmic).

⸻

4.2 Stage 2: Delta Computation

The delta MUST satisfy:

delta = minimal set of nodes that B needs 
         to become causally consistent with A

The delta is a set of root nodes; B will request full subgraphs.

Peers MUST support:
	•	prefix indexing
	•	hash queries
	•	chunked node lists
	•	topological ordering queries

⸻

4.3 Stage 3: Subgraph Transfer

Transfer MUST be:
	•	chunked
	•	deduplicated
	•	content-addressed
	•	BLAKE3-verified
	•	resumable

Each subgraph MUST be:
	•	validated
	•	appended to DAG
	•	indexed
	•	WAL-logged

No mutation allowed — all new truth accumulates.

⸻

5. Ref Synchronization

After DAG sync, references MUST be synchronized:

Rules:

5.1 Fast-Forward Ref Update

If remote ref target is a DAG descendant of local target:

local_ref = remote_ref

5.2 Divergent Ref Update

If both refs diverged:
	•	perform inversion rewrite
	•	produce rewrite node
	•	update ref to merged snapshot

5.3 Tag Handling

Tags MUST NOT move.
If remote tag conflicts:
	•	local tag remains
	•	remote tag stored under distinct namespace

⸻

6. DAG Validation

Every received node MUST be validated:
	1.	BLAKE3 hash
	2.	canonical encoding
	3.	parent existence
	4.	type validation
	5.	rewrite rules (if rewrite node)
	6.	provenance correctness

Invalid nodes MUST be rejected.

⸻

7. Cold Storage Integration

Nodes fetched from remote storage MUST:
	•	be decompressed
	•	be rehydrated
	•	be promoted to warm or hot tier
	•	be indexed
	•	be checked against WAL

This ensures local replicas remain consistent.

⸻

8. Performance Requirements
	•	Frontier exchange < 20ms
	•	DAG delta computation < O(n) in frontier size
	•	Subgraph transfer parallelized
	•	Cold storage fetch pipelined
	•	Rehydration amortized

⸻

9. Security Considerations
	•	signature verification (optional now, mandatory later)
	•	MITM protection
	•	authorized ref updates
	•	denylist of malicious nodes
	•	remote capability negotiation

DAG cannot be polluted.

⸻

10. Failure Modes

10.1 Partial Sync

Must resume without corruption.

10.2 Ref Conflict

Must resolve through inversion, not mutation.

10.3 Node Corruption

Must reject and quarantine.

10.4 Protocol Divergence

Must fallback to safe mode.

⸻

11. Why Distributed Sync Matters

Because JIT is:
	•	global
	•	multi-user
	•	agent-native
	•	eventually-consistent
	•	append-only
	•	immutable
	•	causally structured

Distributed sync is the mechanism by which:
	•	universes converge
	•	agents collaborate
	•	worldlines integrate
	•	truth expands
	•	knowledge flows
	•	replicas remain consistent

This RFC defines the physics of synchronization.

Without it, JIT cannot exist across multiple machines.

⸻

12. Status & Next Steps

This completes the first block of foundational RFCs.

---

# **CΩMPUTER • JITOS** 
© 2025 James Ross • [Flying • Robots](https://flyingrobots.dev)
All Rights Reserved

